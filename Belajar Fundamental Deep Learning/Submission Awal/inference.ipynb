{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Inference Model Terbaik"
      ],
      "metadata": {
        "id": "zvR5F2VifkG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Program ini menganalisis sentimen teks menggunakan model CNN + LSTM. Teks yang dimasukkan oleh pengguna akan melalui tahap preprocessing, seperti penghapusan URL, tanda baca, stopwords, serta konversi ke huruf kecil. Setelah itu, teks dikonversi menjadi vektor numerik menggunakan TF-IDF, dinormalisasi dengan StandardScaler, lalu diproses oleh model Random Forest untuk menentukan sentimennya. Jika hasil prediksi menunjukkan 1, maka sentimen positif (‚úÖ POSITIF), sedangkan jika 0, maka sentimen negatif (‚ùå NEGATIF). Dengan kombinasi preprocessing yang baik, representasi fitur yang optimal, serta model yang telah disempurnakan, program ini mampu memberikan hasil analisis sentimen dengan akurasi tinggi. üöÄ"
      ],
      "metadata": {
        "id": "o8jmA913fhM-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anW26N3sffyJ",
        "outputId": "93c2f705-2115-4c7f-9df5-9e9676cc9841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukkan kalimat baru: aplikasinya biasa saja\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 518ms/step\n",
            "‚úÖ Sentimen kalimat baru adalah NEGATIVE_NEUTRAL.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Download stopwords (if not already downloaded)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the trained CNN+LSTM model\n",
        "model_cnn_lstm = load_model('/content/text_classification_CNN+LSTM_model.h5')  # Update path as needed\n",
        "\n",
        "# Define the tokenizer and max sequence length used during training\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "max_sequence_length = 100  # Replace with the max_sequence_length used during training\n",
        "\n",
        "# Assuming tokenizer is trained on the training dataset (if you're reusing the same tokenizer)\n",
        "# tokenizer.fit_on_texts(X_train)  # Uncomment and train on the original dataset if needed\n",
        "\n",
        "# Preprocessing functions\n",
        "def cleaningText(text):\n",
        "    # Remove URL\n",
        "    text = re.sub(r'https\\S+', ' ', text, flags=re.IGNORECASE)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove mentions (@)\n",
        "    text = re.sub(r'@\\S+', ' ', text, flags=re.IGNORECASE)\n",
        "    # Remove hashtags (#)\n",
        "    text = re.sub(r'#\\S+', ' ', text, flags=re.IGNORECASE)\n",
        "    # Remove any special characters or unwanted symbols\n",
        "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def casefoldingText(text):\n",
        "    return text.lower()\n",
        "\n",
        "def fix_slangwords(text):\n",
        "    # Add slang replacement logic here if necessary\n",
        "    return text\n",
        "\n",
        "def tokenizingText(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def filteringText(text):\n",
        "    stop = stopwords.words('indonesian')\n",
        "    return [word for word in text if word not in stop]\n",
        "\n",
        "def toSentence(list_words):\n",
        "    return ' '.join(list_words)\n",
        "\n",
        "# Function to prepare text for CNN+LSTM model\n",
        "def prepare_input_for_cnn_lstm(text, tokenizer, max_sequence_length):\n",
        "    # Tokenization and padding\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "\n",
        "    # Check if sequence is empty and handle it\n",
        "    if sequence == [[]]:  # If the sequence is empty after tokenization\n",
        "        # You can either return an array of zeros or handle it differently\n",
        "        # Here, we return an array of zeros with the correct shape\n",
        "        padded_sequence = np.zeros((1, max_sequence_length), dtype=np.int32)\n",
        "    else:\n",
        "        # Replace None values with 0 before padding\n",
        "        sequence = [[x if x is not None else 0 for x in sublist] for sublist in sequence]\n",
        "        padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, dtype=np.int32) # Ensure dtype is specified\n",
        "\n",
        "    return padded_sequence\n",
        "\n",
        "# Get user input\n",
        "kalimat_baru = input(\"Masukkan kalimat baru: \")\n",
        "\n",
        "# Preprocess the input text\n",
        "kalimat_baru_cleaned = cleaningText(kalimat_baru)\n",
        "kalimat_baru_casefolded = casefoldingText(kalimat_baru_cleaned)\n",
        "kalimat_baru_slangfixed = fix_slangwords(kalimat_baru_casefolded)\n",
        "kalimat_baru_tokenized = tokenizingText(kalimat_baru_slangfixed)\n",
        "kalimat_baru_filtered = filteringText(kalimat_baru_tokenized)\n",
        "kalimat_baru_final = toSentence(kalimat_baru_filtered)\n",
        "\n",
        "# Prepare the input text for the CNN+LSTM model\n",
        "X_kalimat_baru = prepare_input_for_cnn_lstm(kalimat_baru_final, tokenizer, max_sequence_length)\n",
        "\n",
        "# Make predictions using the CNN+LSTM model\n",
        "prediksi_sentimen = model_cnn_lstm.predict(X_kalimat_baru)\n",
        "\n",
        "# Get the index of the class with the highest probability\n",
        "predicted_class_index = np.argmax(prediksi_sentimen[0])\n",
        "\n",
        "# Map the class index to sentiment label with \"NEGATIVE_NEUTRAL\" combined\n",
        "if predicted_class_index == 0 or predicted_class_index == 1:\n",
        "    sentiment_label = \"NEGATIVE_NEUTRAL\"\n",
        "else:\n",
        "    sentiment_label = \"POSITIVE\"\n",
        "\n",
        "# Display the result\n",
        "print(f\"‚úÖ Sentimen kalimat baru adalah {sentiment_label}.\")"
      ]
    }
  ]
}